%
% This file is part of ICTP RegCM model.
% Copyright (C) 2011 ICTP Trieste
% See the file COPYING for copying conditions.
%

The first step to run a test simulation is to obtain static data to localize
model DOMAIN and Atmosphere and Ocean global model dataset to build initial
and boundary conditions ICBC to run a local area simulation.

ICTP maintains a public accessible web repository of datasets on:

{\bf http://clima-dods.ictp.it/data/d8/cordex }

We will in the following substitute this URL with a shell variable:

\begin{Verbatim}
$> export ICTP_DATASITE=http://clima-dods.ictp.it/data/d8/cordex
\end{Verbatim}

As of now you are requested to download required global data on your local disk
storage before any run attempt. In the future, the ICTP ESP team has
planned to make available an OpenDAP THREDDS Server to give remote access
to global dataset for creating DOMAIN and ICBC without the need to
download the global dataset, but just the required subset in space and time,
using the ICTP web server capabilities to create that subset.

Our advice to you is to use handy transfer program such as uget, but below
we will show you how to use command line download tools curl and
wget to get data.

\section{Global dataset directory Layout}

You are suggested to establish a convenient location for global datasets
on your local storage. Keep in mind that required space for a year of global
data can be as large as 8 GBytes.

Having this in mind, we will now consider that you the user have identified
on your system or have network access to such a storage resource to store say
100 GB of data, and have it reachable on your system under the
\verb=$REGCM_GLOBEDAT= location.
On this directory, you are required to make the following directories:

\begin{Verbatim}
$> cd $REGCM_GLOBEDAT
$> mkdir SURFACE CLM CLM45 SST EIN15
\end{Verbatim}

This does not fill all possible global data sources paths, but will be enough
for the scope of running the model for testing its capabilities.

\section{Static Surface Dataset}

The model needs to be localized on a particular DOMAIN. The needed information
are topography, land type classification and optionally lake depth (to run the
Hostetler lake model) and soil texture classification (to run the chemistry option
with DUST enabled).

This means downloading four files, which are global archives at $30 second$
horizontal resolution on a global latitude-longitude grid of the above data.

\begin{Verbatim}
$> cd $REGCM_GLOBEDAT
$> cd SURFACE
$> curl -o GTOPO_DEM_30s.nc ${ICTP_DATASITE}/SURFACE/GTOPO_DEM_30s.nc
$> curl -o GLCC_BATS_30s.nc ${ICTP_DATASITE}/SURFACE/GLCC_BATS_30s.nc
\end{Verbatim}

Optional Lake and Texture datasets:

\begin{Verbatim}
$> cd $REGCM_GLOBEDAT
$> cd SURFACE
$> curl -o ETOPO_BTM_30s.nc ${ICTP_DATASITE}/SURFACE/ETOPO_BTM_30s.nc
$> curl -o GLZB_SOIL_30s.nc ${ICTP_DATASITE}/SURFACE/GLZB_SOIL_30s.nc
\end{Verbatim}

\section{CLM Dataset}
\label{clmdata}

If you are planning to enable the \verb=CLM= option in the model, you will need
a series of files with global land surface characteristics datasets.

\begin{Verbatim}
$> cd $REGCM_GLOBEDAT
$> cd CLM
$> curl -o mksrf_fmax.nc ${ICTP_DATASITE}/CLM/mksrf_fmax.nc
$> curl -o mksrf_glacier.nc ${ICTP_DATASITE}/CLM/mksrf_glacier.nc
$> curl -o mksrf_lai.nc ${ICTP_DATASITE}/CLM/mksrf_lai.nc
$> curl -o mksrf_lanwat.nc ${ICTP_DATASITE}/CLM/mksrf_lanwat.nc
$> curl -o mksrf_navyoro_20min.nc ${ICTP_DATASITE}/CLM/mksrf_navyoro_20min.nc
$> curl -o mksrf_pft.nc ${ICTP_DATASITE}/CLM/mksrf_pft.nc
$> curl -o mksrf_soicol_clm2.nc ${ICTP_DATASITE}/CLM/mksrf_soicol_clm2.nc
$> curl -o mksrf_soitex.10level.nc ${ICTP_DATASITE}/CLM/mksrf_soitex.10level.nc
$> curl -o mksrf_urban.nc ${ICTP_DATASITE}/CLM/mksrf_urban.nc
$> curl -o pft-physiology.c070207 ${ICTP_DATASITE}/CLM/pft-physiology.c070207
$> curl -o pft-physiology.c070207.readme \
> ${ICTP_DATASITE}/CLM/pft-physiology.c070207.readme
$> curl -o rdirc.05.061026 ${ICTP_DATASITE}/CLM/rdirc.05.061026
\end{Verbatim}

This is the input file for the \verb=clm2rcm= program (see at \ref{clm}).

\section{CLM 4.5 Dataset}
\label{clm45data}

If you are planning to enable the \verb=CLM 4.5= option in the model, you will
need a series of files with global land surface characteristics datasets.

\begin{Verbatim}
$> cd $REGCM_GLOBEDAT
$> cd CLM45
$> mkdir megan pftdata snicardata surface
$> for dir in megan pftdata snicardata surface; do cd $dir; \
    wget ${ICTP_DATASITE}/CLM45/$dir/ -O - | \
    wget -A ".nc" -l1 --no-parent --base=${ICTP_DATASITE}/CLM45/$dir/ \
      -nd -Fri -; done
\end{Verbatim}

This is the input file for the \verb=mksurfdata= program (see at \ref{clm45}).

\section{Sea Surface Temperature}

The model needs a global SST dataset to feed the model with ocean temperature.
You have multiple choices for SST data, but we will for now for our test run
download just CAC OISST weekly for the period 1981 - present.

\begin{Verbatim}
$> cd $REGCM_GLOBEDAT
$> cd SST
$> CDCSITE="ftp.cdc.noaa.gov/pub/Datasets/noaa.oisst.v2"
$> curl -o sst.wkmean.1981-1989.nc \
> ftp://$CDCSITE/sst.wkmean.1981-1989.nc
$> curl -o sst.wkmean.1990-present.nc \
> ftp://$CDCSITE/sst.wkmean.1990-present.nc
\end{Verbatim}

\section{Atmosphere and Land temperature Global Dataset}

The model needs to build initial and boundary conditions for the regional scale,
interpolating on the RegCM grid the data from a Global Climatic Model output.
The GCM dataset can come from any of the supported models, but we will for now
for our test run download just the EIN15 dataset for the year 1990
(Jan 01 00:00:00 UTC to Dec 31 18:00:00 UTC)

\begin{Verbatim}
$> cd $REGCM_GLOBEDAT
$> cd EIN15
$> mkdir 1990
$> cd 1990
$> for type in "air hgt rhum uwnd vwnd"
>  do
>    for hh in "00 06 12 18"
>    do
>      curl -o ${type}.1990.${hh}.nc \
>         ${ICTP_DATASITE}/EIN15/1990/${type}.1990.${hh}.nc
>    done
>  done
\end{Verbatim}

With this datasets we are now ready to go through the RegCM Little Tutorial
in the next chapter of this User Guide.
